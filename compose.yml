# Created to run the Development LLM Environment
name: eo-llm-dev-01

services:
  frontend:
    container_name: openwebui-01
    hostname: openwebui-01
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "8080:8080"
    volumes:
      - ./llm-fe/openwebui:/app/backend/data
    restart: unless-stopped

  backend:
    container_name: ollama-01
    hostname: ollama-01
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ./llm-be/ollama:/root/.ollama
    restart: unless-stopped
  
  pipeline:
    container_name: pipeline-01
    hostname: pipeline-01
    image: ghcr.io/open-webui/pipelines:main
    ports:
      - "9099:9099"
    volumes:
      - ./llm-pl:/app/pipelines
    restart: unless-stopped
    # Workaround python3 -m pip install nltk==3.9b1
